{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Misc\n",
    "# !pip install pymongo\n",
    "# !pip install emoji\n",
    "# !pip install spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import dns\n",
    "import ssl\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_from_Mongo():\n",
    "    client = pymongo.MongoClient(\"mongodb+srv://HAAQ:BigDataProgramming2@cluster0.p7f2o8h.mongodb.net/?retryWrites=true&w=majority\")\n",
    "    records = client.get_database('BD2').Tweets_v1\n",
    "    df = pd.DataFrame.from_dict(records.find())\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Created At</th>\n",
       "      <th>User</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63136388314a50a664fa47cb</td>\n",
       "      <td>Scientists discover a black hole with unstoppa...</td>\n",
       "      <td>2022-09-03 13:42:00</td>\n",
       "      <td>konstructivizm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63136389314a50a664fa47cc</td>\n",
       "      <td>NASA\\nPluto's atmosphere backlit by the Sun, p...</td>\n",
       "      <td>2022-09-03 13:26:00</td>\n",
       "      <td>konstructivizm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63136389314a50a664fa47cd</td>\n",
       "      <td>The Twin Jet Nebula\\nImage credit: NSA/ESA/Hub...</td>\n",
       "      <td>2022-09-03 12:37:00</td>\n",
       "      <td>konstructivizm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63136389314a50a664fa47ce</td>\n",
       "      <td>The Bubble Nebula, also known as NGC 7635, is ...</td>\n",
       "      <td>2022-09-03 12:20:00</td>\n",
       "      <td>konstructivizm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63136389314a50a664fa47cf</td>\n",
       "      <td>A glittering gathering of stars\\n This spectac...</td>\n",
       "      <td>2022-09-03 11:17:00</td>\n",
       "      <td>konstructivizm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>6325e15f38355c7dfc0f3722</td>\n",
       "      <td>NEW\\nJWST vs Hubble of the Cartwheel Galaxy\\nC...</td>\n",
       "      <td>2022-09-14 18:10:00</td>\n",
       "      <td>konstructivizm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>6325e15f38355c7dfc0f3723</td>\n",
       "      <td>New\\nStealth black hole discovery sheds light ...</td>\n",
       "      <td>2022-09-14 17:46:13</td>\n",
       "      <td>konstructivizm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>6325e15f38355c7dfc0f3724</td>\n",
       "      <td>RT @lifeatgoogle: The Grace Hopper Celebration...</td>\n",
       "      <td>2022-09-14 16:32:40</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>6325e15f38355c7dfc0f3725</td>\n",
       "      <td>For people outside the U.S., help is available...</td>\n",
       "      <td>2022-09-14 14:59:13</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>6325e15f38355c7dfc0f3726</td>\n",
       "      <td>Help is a search away.\\n\\nIf you or someone yo...</td>\n",
       "      <td>2022-09-14 14:59:12</td>\n",
       "      <td>Google</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>505 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          _id  \\\n",
       "0    63136388314a50a664fa47cb   \n",
       "1    63136389314a50a664fa47cc   \n",
       "2    63136389314a50a664fa47cd   \n",
       "3    63136389314a50a664fa47ce   \n",
       "4    63136389314a50a664fa47cf   \n",
       "..                        ...   \n",
       "500  6325e15f38355c7dfc0f3722   \n",
       "501  6325e15f38355c7dfc0f3723   \n",
       "502  6325e15f38355c7dfc0f3724   \n",
       "503  6325e15f38355c7dfc0f3725   \n",
       "504  6325e15f38355c7dfc0f3726   \n",
       "\n",
       "                                                Tweets          Created At  \\\n",
       "0    Scientists discover a black hole with unstoppa... 2022-09-03 13:42:00   \n",
       "1    NASA\\nPluto's atmosphere backlit by the Sun, p... 2022-09-03 13:26:00   \n",
       "2    The Twin Jet Nebula\\nImage credit: NSA/ESA/Hub... 2022-09-03 12:37:00   \n",
       "3    The Bubble Nebula, also known as NGC 7635, is ... 2022-09-03 12:20:00   \n",
       "4    A glittering gathering of stars\\n This spectac... 2022-09-03 11:17:00   \n",
       "..                                                 ...                 ...   \n",
       "500  NEW\\nJWST vs Hubble of the Cartwheel Galaxy\\nC... 2022-09-14 18:10:00   \n",
       "501  New\\nStealth black hole discovery sheds light ... 2022-09-14 17:46:13   \n",
       "502  RT @lifeatgoogle: The Grace Hopper Celebration... 2022-09-14 16:32:40   \n",
       "503  For people outside the U.S., help is available... 2022-09-14 14:59:13   \n",
       "504  Help is a search away.\\n\\nIf you or someone yo... 2022-09-14 14:59:12   \n",
       "\n",
       "               User  \n",
       "0    konstructivizm  \n",
       "1    konstructivizm  \n",
       "2    konstructivizm  \n",
       "3    konstructivizm  \n",
       "4    konstructivizm  \n",
       "..              ...  \n",
       "500  konstructivizm  \n",
       "501  konstructivizm  \n",
       "502          Google  \n",
       "503          Google  \n",
       "504          Google  \n",
       "\n",
       "[505 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=fetch_data_from_Mongo()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get today \n",
    "today = date.today()\n",
    "file_name = ('Twitter_realtime_rawdata_'+ str(today))\n",
    "df.to_csv(file_name +'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df):    \n",
    "    # converting the created_time column to datetime datatype.\n",
    "    df['Created At'] = pd.to_datetime(df['Created At'],utc=True)\n",
    "    # Remove Unnamed column\n",
    "    del df['Unnamed: 0']\n",
    "    # Remove duplicated\n",
    "    df['dup'] = df.duplicated(subset=None, keep='first')\n",
    "    # removing the duplicate columns.\n",
    "    df = df[df['dup'] == False]\n",
    "    # Delete duplicated column\n",
    "    del df['dup']\n",
    "    # Check data null or not\n",
    "    df['Tweets'].isnull().values.any()\n",
    "    # Remove na\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "df=data_cleaning(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_all(text):\n",
    "    \n",
    "    dict_country={\n",
    "' ENGGER ':' england versus germany ',\n",
    "' FRAGER ':' france versus germany ',\n",
    "' GERENG ':' germany versus england ',\n",
    "' GERFRA ':' germany versus france ',\n",
    "' GERHUN ':' germany versus hungary ',\n",
    "' GERENG ':' germany versus england ',\n",
    "' GERFRA ':' germany versus france ',\n",
    "' ENGGER ':' germany versus england ',\n",
    "' FRAGER ':' germany versus france ',\n",
    "' HUNGER ':' germany versus hungary ',\n",
    "' ENGVSGER ':' england versus germany ',\n",
    "' FRAVSGER ':' france versus germany ',\n",
    "' GERVSENG ':' germany versus england ',\n",
    "' GERVSFRA ':' germany versus france ',\n",
    "' GERVSHUN ':' germany versus hungary ',\n",
    "' GERVSENG ':' germany versus england ',\n",
    "' GERVSFRA ':' germany versus france  ',\n",
    "' ENGVSGER ':' germany versus england  ',\n",
    "' FRAVSGER ':' germany versus france ',\n",
    "' HUNVSGER ':' germany versus hungary ',\n",
    "' GERNED ':' germany versus Netherlands ',\n",
    "' GERBLR ':' germany versus belarus ',\n",
    "' GEREST ':' germany versus estonia ',\n",
    "' GERNIR ':' germany versus northern ireland ',\n",
    "' GERPOR ':' germany vs portugal ',\n",
    "' NEDGER ':' germany versus Netherlands ',\n",
    "' BLRGER ':' germany versus belarus ',\n",
    "' ESTGER ':' germany versus estonia ',\n",
    "' NIRGER ':' germany versus northern ireland ',\n",
    "' PORGER ':' germany vs portugal ',\n",
    "' GERVSNED ':' germany versus Netherlands ',\n",
    "' GERVSBLR ':' germany versus belarus ',\n",
    "' GERVSEST ':' germany versus estonia ',\n",
    "' GERVSNIR ':' germany versus northern ireland ',\n",
    "' GERVSPOR ':' germany vs portugal ',\n",
    "' NEDVSGER ':' germany versus Netherlands ',\n",
    "' BLRVSGER ':' germany versus belarus ',\n",
    "' ESTVSGER ':' germany versus estonia ',\n",
    "' NIRVSGER ':' germany versus northern ireland ',\n",
    "' PORVSGER ':' germany vs portugal ',\n",
    "' GERMANYKKKKKKKKKKKKK ':' germany ',\n",
    "' GERMANYYY ':' germany ',\n",
    "' GERMANYYYYY ':' germany ',\n",
    "' GERMAY ':' germany ',\n",
    "' Germ ':' germany ',\n",
    "' Germa ':' germany ',\n",
    "' German ':' germany ',\n",
    "' Germani ':' germany ',\n",
    "' Germania ':' germany ',\n",
    "' Germanieee ':' germany ',\n",
    "' Germanies ':' germany ',\n",
    "' Germans ':' germany ',\n",
    "' Germany ':' germany ',\n",
    "' Germany- ':' germany ',\n",
    "' Germany-1 ':' germany ',\n",
    "' Germany-2 ':' germany ',\n",
    "' Germany/ ':' germany ',\n",
    "' Germany1 ':' germany ',\n",
    "' Germany4 ':' germany ',\n",
    "' GermanyVsHungary ':' germany ',\n",
    "' Germanycomeback ':' germany ',\n",
    "' Germanygirls ':' germany ',\n",
    "' Germanynis ':' germany ',\n",
    "' Germanys ':' germany ',\n",
    "' Germanyyyy ':' germany ',\n",
    "' Germanyyyyy ':' germany ',\n",
    "' Germanyyyyyyyyy ':' germany ',\n",
    "' Germanz ':' germany ',\n",
    "' Germeny ':' germany ',\n",
    "' Geramany ':' germany ',\n",
    "\" Germany's \":' germany ',\n",
    "' germany ':' germany ',\n",
    "' Germanys ':' germany ',\n",
    "' TeamGermany ':' germany ',\n",
    "' GERMANE ':' germany ',\n",
    "' GER\\s ':' germany ',\n",
    "' england ':' england ',\n",
    "' ENG\\s ':' england ',\n",
    "' TeamEngland ':' england ',\n",
    "' TeamHungary ':' hungary ',\n",
    "' hungary ':' hungary ',\n",
    "' HUN\\s ':' hungary ',\n",
    "' Hungria ':' hungary ',\n",
    "' TeamItaly ':' italy ',\n",
    "' italy ':' italy ',\n",
    "' ITA\\s ':' italy ',\n",
    "' Ital ':' italy ',\n",
    "' itlay ':' italy ',\n",
    "' TeamFrance ':' france ',\n",
    "' france ':' france ',\n",
    "' FRA\\s ':' france ',\n",
    "' TeamNetherlands ':' netherlands ',\n",
    "' netherlands ':' netherlands ',\n",
    "' NED\\s ':' netherlands  ',\n",
    "' belarus ':' belarus ',\n",
    "' BLR\\s ':' belarus ',\n",
    "' estonia ':' estonia ',\n",
    "' EST\\s ':' estonia ',\n",
    "' NIR\\s ':' northern Ireland ',\n",
    "' portugal ':' portugal ',\n",
    "' POR\\s ':' portugal ',\n",
    "' Nether ':' netherlands  ',\n",
    "' Netherland ':' netherlands  ',\n",
    "' Portugalllll ':' portugal ',\n",
    "' Portugals ':' portugal ',\n",
    "' TeamPortugal ':' portugal ',\n",
    "' GoPortugal ':' portugal ',\n",
    "' Portugal ':' portugal ',\n",
    "' EURO2020\\s ':' eurocup ',\n",
    "' EURO2020.\\s ':' eurocup ',\n",
    "\n",
    "}\n",
    "    \n",
    "    for i, j in dict_country.items():\n",
    "        text = re.sub(i, j,text,flags=re.IGNORECASE)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_comments']=df['Tweets'].apply(replace_all).str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import en_core_web_sm\n",
    "nlp2 = en_core_web_sm.load()\n",
    "\n",
    "nlp = English()\n",
    "#nlp2= spacy.load('en_core_web_lg')\n",
    "tokenizer = nlp.tokenizer\n",
    "s_stemmer=SnowballStemmer(language='english')\n",
    "from emoji import demojize\n",
    "\n",
    "def preprocess(input_text):\n",
    "    \n",
    "    # replace emojis with its respective emotion\n",
    "    demojized_text= demojize(input_text)\n",
    "    \n",
    "    # remove the @mentions fromt the text\n",
    "    pattern=\"@\\w+\"\n",
    "    text_mentions_removed=re.sub(pattern,'',demojized_text)\n",
    "    \n",
    "    # remove the web links in the text\n",
    "    http_pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    text_http_removed = http_pattern.sub('', text_mentions_removed)\n",
    "    \n",
    "    # regular expression keeping only letters\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", text_http_removed)\n",
    "    \n",
    "       \n",
    "    # convert to lower case\n",
    "    text_to_lower_case= letters_only_text.lower()\n",
    "    \n",
    "    \n",
    "    # word tokenization\n",
    "    token_list = []\n",
    "    tokens = tokenizer(text_to_lower_case)\n",
    "    for token in tokens:\n",
    "        token_list.append(token.text)\n",
    "    \n",
    "    \n",
    "    #stop words removal\n",
    "    stop_words_removed_text=[]\n",
    "    for tok in token_list:\n",
    "        if nlp.vocab[tok].is_stop == False and len(tok.strip())!=0 and len(tok)!=1:\n",
    "            stop_words_removed_text.append(tok)\n",
    "#     print(stop_words_removed_text)\n",
    "\n",
    "\n",
    "#      stemming\n",
    "#     stemmed_text=[]\n",
    "#     for word in stop_words_removed_text:\n",
    "#         stemmed_text.append(s_stemmer.stem(word))\n",
    "#      print(stemmed_text)\n",
    "    \n",
    "    \n",
    "    #Lemmatization\n",
    "    lemmatized_text=[]\n",
    "    string=''\n",
    "    for w in stop_words_removed_text:\n",
    "        if string=='':\n",
    "            string = w\n",
    "        else:\n",
    "            string= string + ' ' + w \n",
    "#     print(string)\n",
    "    nlp_string = nlp2(string)\n",
    "    for word in nlp_string:\n",
    "        lemmatized_text.append(word.lemma_)\n",
    "#     print(lemmatized_text)\n",
    "    \n",
    "\n",
    "    # converting list back to string\n",
    "    return \" \".join(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the preprocessed objective is appended to the project_df dataframe.\n",
    "df['clean_comments'] = df['clean_comments'].apply(preprocess)\n",
    "# remove the empty string values from the dataframe.\n",
    "df_clean = df[df['clean_comments'] != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(df):\n",
    "    sid=SentimentIntensityAnalyzer()\n",
    "    df['sentiment_scores']= df['clean_comments'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "    df['sentiment']= df['sentiment_scores'].apply(lambda y: 'Positive' if y>=0 else 'Negative')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final= sentiment_analysis(df_clean)\n",
    "df_final.sentiment.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02d42c9b4c0b801aaebb2f515513dbba2222c794c91929b2482cbece47c9ec34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
